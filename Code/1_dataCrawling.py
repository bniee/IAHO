# -*- coding: utf-8 -*-
"""1_Crawling_codeFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uW0ja-0pgOiRISX1I-A58hTP7hhHADki

### 라이브러리 설치
"""

!pip install selenium
import requests
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import time
import datetime
import warnings
warnings.filterwarnings('ignore')

from bs4 import BeautifulSoup
from selenium import webdriver

from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from urllib.request import urlopen
from urllib.parse import quote_plus

options = Options()
options.add_argument('--disable-gpu');
options.add_argument('--disable-extensions');
options.add_argument('--proxy-server="direct://"');
options.add_argument('--proxy-bypass-list=*');
options.add_argument('--start-maximized');

"""### 기본 웹크롤링 코드 ###"""

url = "https://www.csi.go.kr/acd/acdCaseList.do"

service = Service()
options = webdriver.ChromeOptions()
driver = webdriver.Chrome(service=service, options=options)
driver.get(url)
driver.implicitly_wait(1)

report_df = pd.DataFrame(columns=("사고명", "발생일시", "사고인지 시간", "공공/민간", "기상상태", "시설물 종류", "인적사고", "물적사고", "보호조치여부", "공종", "사고객체", "작업프로세스", "장소", "부위","사고 경위", "사고 원인", "구체적 사고 원인", "사망자수", "부상자수", "피해금액", "피해내용", "사고신고사유","조치사항", "재발방지대책", "공사비", "낙찰률", "공사기간", "공정률", "작업자수", "안전관리계획", "설계안전성검토", "사고조사방법", "향후조치계획", "URL"))
idx = 0

start = 1
page_num = 130  #1~10페이지 = page_num:1 => 최근 데이터 업데이트를 위해 340까지

for n in range(58):
        driver.find_element('xpath','//*[@id="main"]/div/div[2]/div/ul/li[13]/a').click()

while 1:
    if start > page_num:
            break

    for k in range(1, 10):  #1~9페이지 사례 수집

        for i in range(1, 11): #한 페이지 내 목록에서 10개 사례 수집
            driver.find_element('xpath','//*[@id="main"]/div/div[1]/table/tbody/tr[' + str(i) + ']/td[2]/a').click() #사고번호 클릭
            time.sleep(1)

            new_html = driver.page_source
            new_soup = BeautifulSoup(new_html, 'html.parser')

            table = new_soup.find('section', {'id': 'content'}) #사고사례 + 현장특성 + 사고조사 + 현장사진 모든 테이블 불러오기
            data = table.find_all('td', {'class': 't-left'}) #모든 t-left 찾아서 data_list에 추가

            data_list = []

            for m in data:

                data_list.append(m.get_text().strip().replace('\t', '').replace('\n', ''))

            name = data_list[0]
            date = data_list[1]
            check = data_list[2]
            category = data_list[3]
            weather = data_list[4]
            facility = data_list[5]
            human_acc = data_list[6]
            object_acc = data_list[8]
            safety = data_list[7]
            con_type = data_list[9]
            subject = data_list[10]
            process = data_list[11]
            place = data_list[12]
            part = data_list[13]
            case = data_list[14]
            cause = data_list[15]
            detail_cause = data_list[16]
            death = data_list[17]
            injury = data_list[18]
            cost = data_list[19]
            damage = data_list[20]
            report = data_list[21]
            prevention = data_list[22]
            solution = data_list[23]
            project_cost = data_list[25]
            bid_rate = data_list[26]
            duration = data_list[27]
            process_rate = data_list[28]
            workers_num = data_list[29]
            safety_mgmt = data_list[30]
            dfs = data_list[31]
            inspection = data_list[32]
            action = data_list[35]
            u = table.find('div', {'style': 'text-align: end;'}).get_text() #사고사례별 URL

            report_df.loc[idx] = [name, date, check, category, weather, facility, human_acc, object_acc, safety, con_type, subject, process, place, part, case, cause, detail_cause, death, injury, cost, damage, report, prevention, solution, project_cost, bid_rate, duration, process_rate, workers_num, safety_mgmt, dfs, inspection, action, u]
            idx += 1  #엑셀 행 추가

            driver.back() # 목록 페이지로 돌아가기
        driver.find_element('xpath','//*[@id="main"]/div/div[2]/div/ul/li[' + str(k+3) + ']/a').click() # 다음 페이지 숫자 클릭
        time.sleep(1)

    for i in range(1, 11): #마지막 페이지(10)에서 10개 사례 수집
                driver.find_element('xpath','//*[@id="main"]/div/div[1]/table/tbody/tr[' + str(i) + ']/td[2]/a').click() #사고번호 클릭
                time.sleep(1)

                new_html = driver.page_source
                new_soup = BeautifulSoup(new_html, 'html.parser')

                table = new_soup.find('section', {'id': 'content'}) #사고사례 + 현장특성 + 사고조사 + 현장사진 모든 테이블 불러오기
                data = table.find_all('td', {'class': 't-left'}) #모든 t-left 찾아서 data_list에 추가

                data_list = []

                for m in data:

                    data_list.append(m.get_text().strip().replace('\t', '').replace('\n', ''))

                name = data_list[0]
                date = data_list[1]
                check = data_list[2]
                category = data_list[3]
                weather = data_list[4]
                facility = data_list[5]
                human_acc = data_list[6]
                object_acc = data_list[8]
                safety = data_list[7]
                con_type = data_list[9]
                subject = data_list[10]
                process = data_list[11]
                place = data_list[12]
                part = data_list[13]
                case = data_list[14]
                cause = data_list[15]
                detail_cause = data_list[16]
                death = data_list[17]
                injury = data_list[18]
                cost = data_list[19]
                damage = data_list[20]
                report = data_list[21]
                prevention = data_list[22]
                solution = data_list[23]
                project_cost = data_list[25]
                bid_rate = data_list[26]
                duration = data_list[27]
                process_rate = data_list[28]
                workers_num = data_list[29]
                safety_mgmt = data_list[30]
                dfs = data_list[31]
                inspection = data_list[32]
                action = data_list[35]
                u = table.find('div', {'style': 'text-align: end;'}).get_text() #사고사례별 URL

                report_df.loc[idx] = [name, date, check, category, weather, facility, human_acc, object_acc, safety, con_type, subject, process, place, part, case, cause, detail_cause, death, injury, cost, damage, report, prevention, solution, project_cost, bid_rate, duration, process_rate, workers_num, safety_mgmt, dfs, inspection, action, u]
                idx += 1  #엑셀 행 추가

                driver.back() # 목록 페이지로 돌아가기\

    driver.find_element('xpath','//*[@id="main"]/div/div[2]/div/ul/li[13]/a').click() # > 클릭하여 다음 page_num으로 이동
    time.sleep(1)

    start += 1

report_df.shape

report_df

report_df.to_csv('C:/Users/user/Desktop/230705_588~.csv', index=False, encoding="utf-8-sig")

"""### 웹크롤링 페이지 거꾸로 버전 ###
#### - 오류로 웹크롤링이 중단되어 나머지 부분 수집하기 위한 임시 코드입니다..
"""

url = "https://www.csi.go.kr/acd/acdCaseList.do"

service = Service()
options = webdriver.ChromeOptions()
driver = webdriver.Chrome(service=service, options=options)
driver.get(url)
driver.implicitly_wait(1)

report_df = pd.DataFrame(columns=("사고명", "발생일시", "사고인지 시간", "공공/민간", "기상상태", "시설물 종류", "인적사고", "물적사고", "보호조치여부", "공종", "사고객체", "작업프로세스", "장소", "부위","사고 경위", "사고 원인", "구체적 사고 원인", "사망자수", "부상자수", "피해금액", "피해내용", "사고신고사유","조치사항", "재발방지대책", "공사비", "낙찰률", "공사기간", "공정률", "작업자수", "안전관리계획", "설계안전성검토", "사고조사방법", "향후조치계획", "URL"))
idx = 0

start = 1
page_num = 130  #1~10페이지 = page_num:1 => 최근 데이터 업데이트를 위해 340까지

driver.find_element('xpath','//*[@id="main"]/div/div[2]/div/ul/li[14]/a').click() # 맨 끝페이지 가기
driver.find_element('xpath','//*[@id="main"]/div/div[2]/div/ul/li[2]/a').click() # 1920페이지로 이동

while 1:
    if start > page_num:
            break

    for k in range(1, 10):  #1~9페이지 사례 수집

        for i in range(1, 11): #한 페이지 내 목록에서 10개 사례 수집
            driver.find_element('xpath','//*[@id="main"]/div/div[1]/table/tbody/tr[' + str(i) + ']/td[2]/a').click() #사고번호 클릭
            time.sleep(1)

            new_html = driver.page_source
            new_soup = BeautifulSoup(new_html, 'html.parser')

            table = new_soup.find('section', {'id': 'content'}) #사고사례 + 현장특성 + 사고조사 + 현장사진 모든 테이블 불러오기
            data = table.find_all('td', {'class': 't-left'}) #모든 t-left 찾아서 data_list에 추가

            data_list = []

            for m in data:

                data_list.append(m.get_text().strip().replace('\t', '').replace('\n', ''))

            name = data_list[0]
            date = data_list[1]
            check = data_list[2]
            category = data_list[3]
            weather = data_list[4]
            facility = data_list[5]
            human_acc = data_list[6]
            object_acc = data_list[8]
            safety = data_list[7]
            con_type = data_list[9]
            subject = data_list[10]
            process = data_list[11]
            place = data_list[12]
            part = data_list[13]
            case = data_list[14]
            cause = data_list[15]
            detail_cause = data_list[16]
            death = data_list[17]
            injury = data_list[18]
            cost = data_list[19]
            damage = data_list[20]
            report = data_list[21]
            prevention = data_list[22]
            solution = data_list[23]
            project_cost = data_list[25]
            bid_rate = data_list[26]
            duration = data_list[27]
            process_rate = data_list[28]
            workers_num = data_list[29]
            safety_mgmt = data_list[30]
            dfs = data_list[31]
            inspection = data_list[32]
            action = data_list[35]
            u = table.find('div', {'style': 'text-align: end;'}).get_text() #사고사례별 URL

            report_df.loc[idx] = [name, date, check, category, weather, facility, human_acc, object_acc, safety, con_type, subject, process, place, part, case, cause, detail_cause, death, injury, cost, damage, report, prevention, solution, project_cost, bid_rate, duration, process_rate, workers_num, safety_mgmt, dfs, inspection, action, u]
            idx += 1  #엑셀 행 추가

            driver.back() # 목록 페이지로 돌아가기
        driver.find_element('xpath','//*[@id="main"]/div/div[2]/div/ul/li[' + str(12-k) + ']/a').click() # 이전 페이지 숫자 클릭
        time.sleep(1)

    for i in range(1, 11): #마지막 페이지(10)에서 10개 사례 수집
                driver.find_element('xpath','//*[@id="main"]/div/div[1]/table/tbody/tr[' + str(i) + ']/td[2]/a').click() #사고번호 클릭
                time.sleep(1)

                new_html = driver.page_source
                new_soup = BeautifulSoup(new_html, 'html.parser')

                table = new_soup.find('section', {'id': 'content'}) #사고사례 + 현장특성 + 사고조사 + 현장사진 모든 테이블 불러오기
                data = table.find_all('td', {'class': 't-left'}) #모든 t-left 찾아서 data_list에 추가

                data_list = []

                for m in data:

                    data_list.append(m.get_text().strip().replace('\t', '').replace('\n', ''))

                name = data_list[0]
                date = data_list[1]
                check = data_list[2]
                category = data_list[3]
                weather = data_list[4]
                facility = data_list[5]
                human_acc = data_list[6]
                object_acc = data_list[8]
                safety = data_list[7]
                con_type = data_list[9]
                subject = data_list[10]
                process = data_list[11]
                place = data_list[12]
                part = data_list[13]
                case = data_list[14]
                cause = data_list[15]
                detail_cause = data_list[16]
                death = data_list[17]
                injury = data_list[18]
                cost = data_list[19]
                damage = data_list[20]
                report = data_list[21]
                prevention = data_list[22]
                solution = data_list[23]
                project_cost = data_list[25]
                bid_rate = data_list[26]
                duration = data_list[27]
                process_rate = data_list[28]
                workers_num = data_list[29]
                safety_mgmt = data_list[30]
                dfs = data_list[31]
                inspection = data_list[32]
                action = data_list[35]
                u = table.find('div', {'style': 'text-align: end;'}).get_text() #사고사례별 URL

                report_df.loc[idx] = [name, date, check, category, weather, facility, human_acc, object_acc, safety, con_type, subject, process, place, part, case, cause, detail_cause, death, injury, cost, damage, report, prevention, solution, project_cost, bid_rate, duration, process_rate, workers_num, safety_mgmt, dfs, inspection, action, u]
                idx += 1  #엑셀 행 추가

                driver.back() # 목록 페이지로 돌아가기\

    driver.find_element('xpath','//*[@id="main"]/div/div[2]/div/ul/li[2]/a').click() # > 클릭하여 다음 page_num으로 이동
    time.sleep(1)

    start += 1

report_df.to_csv('C:/Users/user/Desktop/230707_1871~1920.csv', index=False, encoding="utf-8-sig")

